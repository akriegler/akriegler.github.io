<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Andreas Kriegler </title> <meta name="author" content="Andreas Kriegler"> <meta name="description" content="PhD Student "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://akriegler.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/activities/">Activities </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Andreas <span class="font-weight-bold">Kriegler</span> </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?1631a9e3d4c5e494a3ca885da68f075e" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>andreas.kriegler@tuwien.ac.at</p> <p>Giefinggasse 4</p> <p>1210 Vienna, Austria</p> </div> </div> <div class="clearfix"> <p>I am a PhD Student in Computer Vision in the <a href="https://cvl.tuwien.ac.at/" rel="external nofollow noopener" target="_blank">CVL</a> at <a href="https://www.tuwien.at/en/" rel="external nofollow noopener" target="_blank">Technical University of Vienna (TU Wien)</a>, supervised by <a href="https://informatics.tuwien.ac.at/people/margrit-gelautz" rel="external nofollow noopener" target="_blank">Margrit Gelautz</a>. I am funded by the <a href="https://www.ait.ac.at/en/" rel="external nofollow noopener" target="_blank">Austrian Institute of Technology (AIT)</a>, where I am further advised by <a href="https://publications.ait.ac.at/de/persons/csaba.beleznai" rel="external nofollow noopener" target="_blank">Csaba Beleznai</a>.</p> <p>My research lies in the intersection of classical Computer Vision, Deep Learning and Computer Graphics. The goal is to develop novel methods for highly generic object pose estimation. We are specifically interested in the geometry of objects and exploit rendering engines to generate large quantities of synthetic data.</p> <p>Before I joined TU Wien, I received both the BSc. and MSc. in Mechatronics/Robotics from the <a href="https://www.technikum-wien.at/en/" rel="external nofollow noopener" target="_blank">University of Applied Sciences Technikum Vienna</a> (with distinction).</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Apr 28, 2025</th> <td> Co-organized the <a href="https://icbinb.cc/" rel="external nofollow noopener" target="_blank">ICBINB</a>: <a href="https://sites.google.com/view/icbinb-2025/home?authuser=0" rel="external nofollow noopener" target="_blank">Challenges in Applied Deep Learning workshop</a> at ICLR 2025. Thank you to authors, reviewers, chairs, speakers, panelists and other organizers for making it a success. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 24, 2025</th> <td> We (CVL - TU Wien) showcased our human pose imitation system using a Pepper robot at the science fair <a href="https://wirtschaftsagentur.at/termine-events-workshops/wiener-forschungsfest-2025/" rel="external nofollow noopener" target="_blank">Forschungsfest 2025</a>, as part of the <a href="https://www.caringrobots.eu" rel="external nofollow noopener" target="_blank">Caring Robots</a> project. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 23, 2024</th> <td> <a href="https://dl.acm.org/doi/10.1145/3682074" rel="external nofollow noopener" target="_blank">Review paper on body movement imitation</a> published in the ACM Transactions on Human-Robot Interaction (THRI). </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 13, 2024</th> <td> Attended the International Computer Vision Summer School (ICVSS 2024) and held a <a href="https://iplab.dmi.unict.it/icvss2024/Posters" rel="external nofollow noopener" target="_blank">poster presentation</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 16, 2023</th> <td> Co-organized the <a href="https://icbinb.cc/" rel="external nofollow noopener" target="_blank">ICBINB</a>: <a href="https://sites.google.com/view/icbinb-2023/home" rel="external nofollow noopener" target="_blank">Failure Modes in the Age of Foundation Models workshop</a> at NeurIPS 2023. Proceedings published in <a href="https://proceedings.mlr.press/v239/" rel="external nofollow noopener" target="_blank">PMLR volume 239</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 09, 2023</th> <td> <a href="https://www.worldscientific.com/doi/10.1142/S1793351X23620027" rel="external nofollow noopener" target="_blank">PrimitivePose</a> paper for 6D object pose estimation via synthetic 3D primitives published in the International Journal of Semantic Computing (IJSC), as part of my PhD. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/stoeva2024-480.webp 480w,/assets/img/publication_preview/stoeva2024-800.webp 800w,/assets/img/publication_preview/stoeva2024-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/stoeva2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="stoeva2024.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Stoeva2024" class="col-sm-8"> <div class="title">Review Paper: Body Movement Mirroring and Synchrony in Human-Robot Interaction</div> <div class="author"> Darja Stoeva, <em>Andreas Kriegler</em>, and Margrit Gelautz </div> <div class="periodical"> <em>ACM Transactions on Human-Robot Interaction (THRI)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3682074" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/stoeva2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xzhMsboAAAAJ&amp;citation_for_view=xzhMsboAAAAJ:0EnyYjriUFMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This review paper provides an overview of papers that have studied body movement mirroring and synchrony within the field of human-robot interaction. The papers included in this review cover system studies, which focus on evaluating the technical aspects of mirroring and synchrony robotic systems, and user studies, which focus on measuring particular interaction outcomes or attitudes towards robots expressing mirroring and synchrony behaviors. We review the papers in terms of the employed robotic platforms and the focus on parts of the body, the techniques used to sense and react to human motion, the evaluation methods, the intended applications of the human-robot interaction systems and the scenarios utilized in user studies. Finally, challenges and possible future directions are considered and discussed.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Stoeva2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Review Paper: Body Movement Mirroring and Synchrony in Human-Robot Interaction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stoeva, Darja and Kriegler, Andreas and Gelautz, Margrit}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Human-Robot Interaction (THRI)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--26}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3682074}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{0EnyYjriUFMC}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association of Computing Machinery}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kriegler2023-480.webp 480w,/assets/img/publication_preview/kriegler2023-800.webp 800w,/assets/img/publication_preview/kriegler2023-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/kriegler2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kriegler2023.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Kriegler2023" class="col-sm-8"> <div class="title">PrimitivePose: Generic Model and Representation for 3D Bounding Box Prediction of Unseen Objects</div> <div class="author"> <em>Andreas Kriegler</em>, Csaba Beleznai, Margrit Gelautz, Markus Murschitz, and Kai Göbel </div> <div class="periodical"> <em>International Journal of Semantic Computing (IJSC)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.worldscientific.com/doi/10.1142/S1793351X23620027" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/kriegler2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://drive.google.com/drive/folders/1CKCv6wGnTBDnKX8corvAhON97DNbQw9d" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/akriegler/PrimitivePose-Annotation-Tool" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xzhMsboAAAAJ&amp;citation_for_view=xzhMsboAAAAJ:UebtZRa9Y70C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>A considerable amount of research is concerned with the challenging task of estimating three-dimensional (3D) pose and size for multi-object indoor scene con¯gurations. Many existing models rely on a priori known object models, such as 3D CAD models and are therefore limited to a prede¯ned set of object categories. This closed-set constraint limits the range of applications for robots interacting in dynamic environments where previously unseen objects may appear. This paper addresses this problem with a highly generic 3D bounding box detection method that relies entirely on geometric cues obtained from depth data percepts. While the generation of synthetic data, e.g. synthetic depth maps, is commonly used for this task, the well-known synth-to-real gap often emerges, which prohibits transition of models trained solely on synthetic data to the real world. To ameliorate this problem, we use stereo depth computation on synthetic data to obtain pseudo-realistic disparity maps. We then propose an intermediate representation, namely disparity-scaled surface normal (SN) images, which encodes geometry and at the same time preserves depth/scale information unlike the commonly used standard SNs. In a series of experiments, we demonstrate the usefulness of our approach, detecting everyday objects on a captured data set of tabletop scenes, and compare it to the popular PoseCNN model. We quantitatively show that standard SNs are less adequate for challenging 3D detection tasks by comparing predictions from the model trained on disparity alone, SNs and disparity-scaled SNs. Additionally, in an ablation study we investigate the minimal number of training samples required for such a learning task. Lastly, we make the tool used for 3D object annotation publicly available at: https://preview.tinyurl.com/3ycn8v5k. A video showcasing our results can be found at: https://preview.tinyurl.com/dzdzabek.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Kriegler2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kriegler, Andreas and Beleznai, Csaba and Gelautz, Margrit and Murschitz, Markus and Göbel, Kai}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PrimitivePose: Generic Model and Representation for 3D Bounding Box Prediction of Unseen Objects}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Semantic Computing (IJSC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{387--410}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1142/S1793351X23620027}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{UebtZRa9Y70C}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{World Scientific Publishing Company}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kriegler2022b-480.webp 480w,/assets/img/publication_preview/kriegler2022b-800.webp 800w,/assets/img/publication_preview/kriegler2022b-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/kriegler2022b.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kriegler2022b.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Kriegler2022b" class="col-sm-8"> <div class="title">Paradigmatic Revolutions in Computer Vision</div> <div class="author"> <em>Andreas Kriegler</em> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems 35 (NeurIPS) - ICBINB: Understanding Deep Learning Through Empirical Falsification</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://drive.google.com/file/d/1ItDGcljAQOYo-HKHJLcC4gAgapCIQPns/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/kriegler2022b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/kriegler2022bposter.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xzhMsboAAAAJ&amp;citation_for_view=xzhMsboAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-3-4285F4?logo=googlescholar&amp;labelColor=beige" alt="3 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Kuhn’s groundbreaking Structure divides scientific progress into four phases, the pre-paradigm period, normal science, scientific crisis and revolution. Most of the time a field advances incrementally, constrained and guided by a currently agreed upon paradigm. Creative phases emerge when phenomena occur which lack satisfactory explanation within the current paradigm (the crisis) until a new one replaces it (the revolution). This model of science was mainly laid out by exemplars from natural science, while we want to show that Kuhn’s work is also applicable for information sciences. We analyze the state of one field in particular, computer vision, using Kuhn’s vocabulary. Following significant technology-driven advances of machine learning methods in the age of deep learning, researchers in computer vision were eager to accept the models that now dominate the state of the art. We discuss the current state of the field especially in light of the deep learning revolution and conclude that current deep learning methods cannot fully constitute a paradigm for computer vision in the Kuhnian sense.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Kriegler2022b</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kriegler, Andreas}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Paradigmatic Revolutions in Computer Vision}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems 35 (NeurIPS) - ICBINB: Understanding Deep Learning Through Empirical Falsification}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{_FxGoFyzp5QC}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kriegler2022c-480.webp 480w,/assets/img/publication_preview/kriegler2022c-800.webp 800w,/assets/img/publication_preview/kriegler2022c-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/kriegler2022c.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kriegler2022c.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Kriegler2022c" class="col-sm-8"> <div class="title">PrimitivePose: 3D Bounding Box Prediction of Unseen Objects via Synthetic Geometric Primitives</div> <div class="author"> <em>Andreas Kriegler</em>, Csaba Beleznai, Markus Murschitz, Kai Göbel, and Margrit Gelautz </div> <div class="periodical"> <em>In International Conference on Robotic Computing (IRC)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10023891" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/kriegler2022c.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://drive.google.com/file/d/1Kp2VM9My9US5zMVosbYEZnct0_UOldkp/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/assets/pdf/kriegler2022cslides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xzhMsboAAAAJ&amp;citation_for_view=xzhMsboAAAAJ:roLk4NBRz8UC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-3-4285F4?logo=googlescholar&amp;labelColor=beige" alt="3 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper studies the challenging problem of 3D pose and size estimation for multi-object scene configurations from stereo views. Most existing methods rely on CAD models and are therefore limited to a predefined set of known object categories. This closed-set constraint limits the range of applications for robots interacting in dynamic environments where previously unseen objects may appear. To address this problem we propose an oriented 3D bounding box detection method that does not require 3D models or semantic information of the objects and is learned entirely from the category-specific domain, relying on purely geometric cues. These geometric cues are objectness and compactness, as represented in the synthetic domain by generating a diverse set of stereo image pairs featuring pose annotated geometric primitives. We then use stereo matching and derive three representations for 3D image content: disparity maps, surface normal images and a novel representation of disparity-scaled surface normal images. The proposed model, PrimitivePose, is trained as a single-stage multi-task neural network using any one of those representations as input and 3D oriented bounding boxes, object centroids and object sizes as output. We evaluate PrimitivePose for 3D bounding box prediction on difficult unseen objects in a tabletop environment and compare it to the popular PoseCNN model – a video showcasing our results can be found at: https://preview.tinyurl.com/2pccumvt.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Kriegler2022c</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kriegler, Andreas and Beleznai, Csaba and Murschitz, Markus and Göbel, Kai and Gelautz, Margrit}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PrimitivePose: 3D Bounding Box Prediction of Unseen Objects via Synthetic Geometric Primitives}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Robotic Computing (IRC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{190--197}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IRC55401.2022.00040}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{roLk4NBRz8UC}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/steininger2021-480.webp 480w,/assets/img/publication_preview/steininger2021-800.webp 800w,/assets/img/publication_preview/steininger2021-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/steininger2021.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="steininger2021.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Steininger2021" class="col-sm-8"> <div class="title">The Aircraft Context Dataset: Understanding and Optimizing Data Variability in Aerial Domains</div> <div class="author"> Daniel Steininger, Verena Widhalm, Julia Simon, <em>Andreas Kriegler</em>, and Christoph Sulzbacher </div> <div class="periodical"> <em>In International Conference on Computer Vision (ICCV) Workshop - 1st Workshop on Airborne Object Tracking</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9607692" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/steininger2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/steininger2021supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/aircraftcontext/aircraft-context-dataset" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xzhMsboAAAAJ&amp;citation_for_view=xzhMsboAAAAJ:d1gkVwhDpl0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-11-4285F4?logo=googlescholar&amp;labelColor=beige" alt="11 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Despite their increasing demand for assistant and autonomous systems, the recent shift towards data-driven approaches has hardly reached aerial domains, partly due to a lack of specific training and test data. We introduce the Aircraft Context Dataset, a composition of two intercompatible large-scale and versatile image datasets focusing on manned aircraft and UAVs, respectively. In addition to fine-grained annotations for multiple learning tasks, we define and apply a set of relevant meta-parameters and showcase their potential to quantify dataset variability as well as the impact of environmental conditions on model performance. Baseline experiments are conducted for detection, classification and semantic labeling on multiple dataset variants. Their evaluation clearly shows that our contribution is an essential step towards overcoming the data gap and that the proposed variability concept significantly increases the efficiency of specializing models as well as continuously and purposefully extending the dataset.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Steininger2021</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Steininger, Daniel and Widhalm, Verena and Simon, Julia and Kriegler, Andreas and Sulzbacher, Christoph}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Aircraft Context Dataset: Understanding and Optimizing Data Variability in Aerial Domains}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision (ICCV) Workshop - 1st Workshop on Airborne Object Tracking}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3823--3832}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICCVW54120.2021.00426}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{d1gkVwhDpl0C}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kriegler2020a-480.webp 480w,/assets/img/publication_preview/kriegler2020a-800.webp 800w,/assets/img/publication_preview/kriegler2020a-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/kriegler2020a.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kriegler2020a.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Kriegler2020a" class="col-sm-8"> <div class="title">Vision-based Docking of a Mobile Robot</div> <div class="author"> <em>Andreas Kriegler</em>, and Wilfried Wöber </div> <div class="periodical"> <em>In Joint Austrian Computer Vision and Robotics Workshop (ACVRW)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openlib.tugraz.at/proceedings-of-the-joint-austrian-computer-vision-and-robotics-workshop-2020-2020" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/kriegler2020a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xzhMsboAAAAJ&amp;citation_for_view=xzhMsboAAAAJ:eQOLeE2rZwMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>For mobile robots to be considered autonomous they must reach target locations in required pose, a procedure referred to as docking. Popular current solutions use LiDARs combined with sizeable docking stations but these systems struggle by incorrectly detecting dynamic obstacles. This paper instead proposes a vision-based framework for docking a mobile robot. Faster R-CNN is used for detecting arbitrary visual markers. The pose of the robot is estimated using the solvePnP algorithm relating 2D-3D point pairs. Following exhaustive experiments, it is shown that solvePnP gives systematically inaccurate pose estimates in the x-axis pointing to the side. Pose estimates are off by ten to fifty centimeters and could therefore not be used for docking the robot. Insights are provided to circumvent similar problems in future applications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Kriegler2020a</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kriegler, Andreas and Wöber, Wilfried}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Vision-based Docking of a Mobile Robot}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Joint Austrian Computer Vision and Robotics Workshop (ACVRW)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6--12}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3217/978-3-85125-752-6-03}</span><span class="p">,</span>
  <span class="na">google_scholar_id</span> <span class="p">=</span> <span class="s">{eQOLeE2rZwMC}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%61%6E%64%72%65%61%73.%6B%72%69%65%67%6C%65%72@%74%75%77%69%65%6E.%61%63.%61%74" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-5653-5181" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=xzhMsboAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Andreas-Kriegler/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=57220009394" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://github.com/akriegler" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://publications.ait.ac.at/en/persons/andreas.kriegler" title="Work" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-briefcase"></i></a> <a href="https://dblp.org/pid/307/8171.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="https://www.zotero.org/vindicate223" title="Zotero" rel="external nofollow noopener" target="_blank"><i class="ai ai-zotero"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Andreas Kriegler. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 28, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>